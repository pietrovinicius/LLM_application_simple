#04/10/2024
#@PLima

# # chatbot_streamlit.py

import streamlit as st
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
import datetime

def agora():
    agora = datetime.datetime.now()
    return agora.strftime("%Y-%m-%d %H:%M:%S")

# --- Configura√ß√£o Inicial ---

# Carrega a chave de API do arquivo .env
load_dotenv()
print(f"{agora()} - Vari√°veis de ambiente carregadas do .env")

# Configura√ß√£o da p√°gina do Streamlit
st.set_page_config(page_title="Chatbot com Mem√≥ria", page_icon="üß†")
st.title("ü§ñ Chatbot com Mem√≥ria")
st.caption("Desenvolvido com LangChain, Google Gemini e Streamlit")

# --- L√≥gica do LangChain com Mem√≥ria ---

# 1. Instanciar o modelo LLM
print(f"{agora()} - Instanciando o modelo LLM")
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")


# 2. Criar o template do prompt com mem√≥ria
# O 'MessagesPlaceholder' √© a pe√ßa chave aqui. Ele diz ao LangChain:
# "Insira o hist√≥rico da conversa aqui".
print(f"{agora()} - Criando o template do prompt com mem√≥ria")
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "Voc√™ √© um assistente prestativo. Responda todas as perguntas da melhor forma poss√≠vel."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)

# 3. Criar e inicializar a mem√≥ria
# A mem√≥ria precisa ser armazenada no 'session_state' do Streamlit
# para que n√£o se perca a cada nova intera√ß√£o do usu√°rio.
print(f"{agora()} - Criando e inicializando a mem√≥ria")
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history", return_messages=True
    )

# 4. Criar a LLMChain
# Note que agora usamos LLMChain, que √© projetada para funcionar com mem√≥ria.
# A sintaxe de pipe (LCEL) tamb√©m pode ser usada, mas LLMChain √© mais expl√≠cita aqui.
print(f"{agora()} - Criando a LLMChain")
conversation_chain = LLMChain(
    llm=llm,
    prompt=prompt_template,
    memory=st.session_state.memory,
    verbose=True # verbose=True nos mostra no console o que a chain est√° pensando
)

# --- Interface Gr√°fica com Streamlit ---

# Inicializa o hist√≥rico de mensagens no session_state se n√£o existir
if "messages" not in st.session_state:
    st.session_state.messages = []
    print(f"{agora()} - Inicializando o hist√≥rico de mensagens")


# Exibe as mensagens antigas no chat
print(f"{agora()} - Exibindo as mensagens antigas no chat")
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Captura a entrada do usu√°rio
print(f"{agora()} - Capturando a entrada do usu√°rio")
if user_input := st.chat_input("Qual a sua pergunta?"):
    # Adiciona a mensagem do usu√°rio ao hist√≥rico e √† tela
    st.session_state.messages.append({"role": "user", "content": user_input})
    print(f"{agora()} - Adicionando a mensagem do usu√°rio ao hist√≥rico e √† tela")
    with st.chat_message("user"):
        st.markdown(user_input)

    # Envia a entrada para a chain e obt√©m a resposta
    print(f"{agora()} - Enviando a entrada para a chain e obtendo a resposta")
    with st.spinner("Pensando..."):
        response = conversation_chain.invoke({"input": user_input})

    # Adiciona a resposta do assistente ao hist√≥rico e √† tela
    st.session_state.messages.append({"role": "assistant", "content": response["text"]})
    print(f"{agora()} - Adicionando a resposta do assistente ao hist√≥rico e √† tela")
    with st.chat_message("assistant"):
        st.markdown(response["text"])